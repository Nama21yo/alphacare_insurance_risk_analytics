{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776bf1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a859b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load the dataset\n",
    "file_path = './data/MachineLearningRating_v3.txt'\n",
    "df = pd.read_csv(file_path, delimiter='|')  # Adjust delimiter if needed\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044dc55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Function to calculate claim frequency, severity, and margin\n",
    "def calculate_risk_metrics(df):\n",
    "    \"\"\"Calculates Claim Frequency, Claim Severity, and Margin.\"\"\"\n",
    "    df['ClaimOccurred'] = (df['TotalClaims'] > 0).astype(int)\n",
    "    df['ClaimFrequency'] = df['ClaimOccurred']\n",
    "    df['ClaimSeverity'] = df['TotalClaims'].where(df['ClaimOccurred'] == 1)\n",
    "    df['Margin'] = df['TotalPremium'] - df['TotalClaims']\n",
    "    return df\n",
    "\n",
    "# Apply calculation\n",
    "df = calculate_risk_metrics(df)\n",
    "df[['TotalPremium', 'TotalClaims', 'ClaimOccurred', 'ClaimFrequency', 'ClaimSeverity', 'Margin']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e664ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: T-test for numerical comparisons\n",
    "def perform_t_test(group1_data, group2_data, alpha=0.05):\n",
    "    \"\"\"Performs independent t-test for numerical data.\"\"\"\n",
    "    stat, p_value = stats.ttest_ind(group1_data.dropna(), group2_data.dropna(), equal_var=False)  # Welch's t-test\n",
    "    return stat, p_value, p_value < alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Chi-squared test for categorical comparisons\n",
    "def perform_chi_squared_test(group1_claims_occurred, group1_no_claims,\n",
    "                             group2_claims_occurred, group2_no_claims, alpha=0.05):\n",
    "    \"\"\"Performs Chi-squared test for Claim Frequency.\"\"\"\n",
    "    contingency_table = np.array([[group1_claims_occurred, group1_no_claims],\n",
    "                                  [group2_claims_occurred, group2_no_claims]])\n",
    "    chi2_stat, p_value, _, _ = stats.chi2_contingency(contingency_table)\n",
    "    return chi2_stat, p_value, p_value < alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61eac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Core function to conduct pairwise risk comparison\n",
    "def conduct_hypothesis_test(df, group_col, alpha=0.05):\n",
    "    results = {}\n",
    "    groups = df[group_col].unique()\n",
    "    if len(groups) < 2:\n",
    "        print(f\"Not enough groups in '{group_col}' to perform tests.\")\n",
    "        return results\n",
    "\n",
    "    group_names = df[group_col].value_counts().index.tolist()\n",
    "    if len(group_names) < 2:\n",
    "        print(f\"Insufficient groups for {group_col} comparison.\")\n",
    "        return results\n",
    "\n",
    "    g1_name, g2_name = group_names[0], group_names[1]\n",
    "    group1_df = df[df[group_col] == g1_name]\n",
    "    group2_df = df[df[group_col] == g2_name]\n",
    "\n",
    "    # 1. Claim Frequency\n",
    "    g1_claims = group1_df['ClaimOccurred'].sum()\n",
    "    g1_no_claims = len(group1_df) - g1_claims\n",
    "    g2_claims = group2_df['ClaimOccurred'].sum()\n",
    "    g2_no_claims = len(group2_df) - g2_claims\n",
    "\n",
    "    chi2_freq, p_freq, reject_freq = perform_chi_squared_test(\n",
    "        g1_claims, g1_no_claims, g2_claims, g2_no_claims, alpha)\n",
    "    results[f'Claim_Frequency_{g1_name}_vs_{g2_name}'] = {\n",
    "        'p_value': p_freq,\n",
    "        'reject_null': reject_freq,\n",
    "        'interpretation': f\"Claim frequency difference between {g1_name} and {g2_name} is {'significant' if reject_freq else 'not significant'}\"\n",
    "    }\n",
    "\n",
    "    # 2. Claim Severity\n",
    "    stat_sev, p_sev, reject_sev = perform_t_test(group1_df['ClaimSeverity'], group2_df['ClaimSeverity'], alpha)\n",
    "    results[f'Claim_Severity_{g1_name}_vs_{g2_name}'] = {\n",
    "        'p_value': p_sev,\n",
    "        'reject_null': reject_sev,\n",
    "        'interpretation': f\"Claim severity difference between {g1_name} and {g2_name} is {'significant' if reject_sev else 'not significant'}\"\n",
    "    }\n",
    "\n",
    "    # 3. Margin\n",
    "    stat_margin, p_margin, reject_margin = perform_t_test(group1_df['Margin'], group2_df['Margin'], alpha)\n",
    "    results[f'Margin_Difference_{g1_name}_vs_{g2_name}'] = {\n",
    "        'p_value': p_margin,\n",
    "        'reject_null': reject_margin,\n",
    "        'interpretation': f\"Margin difference between {g1_name} and {g2_name} is {'significant' if reject_margin else 'not significant'}\"\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcb76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Province test\n",
    "def test_province_risk_differences(df, alpha=0.05):\n",
    "    print(\"\\n--- Testing Risk Differences Across Provinces ---\")\n",
    "    province_results = {}\n",
    "    top_provinces = df['Province'].value_counts().index.tolist()\n",
    "    if len(top_provinces) >= 2:\n",
    "        p1, p2 = top_provinces[0], top_provinces[1]\n",
    "        print(f\"Comparing {p1} vs {p2}...\")\n",
    "        province_results.update(conduct_hypothesis_test(df[df['Province'].isin([p1, p2])], 'Province', alpha))\n",
    "    else:\n",
    "        print(\"Not enough unique provinces for comparison.\")\n",
    "    return province_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77084eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Zipcode risk difference test\n",
    "def test_zipcode_risk_differences(df, alpha=0.05):\n",
    "    print(\"\\n--- Testing Risk Differences Between Zipcodes ---\")\n",
    "    zipcode_results = {}\n",
    "    top_zipcodes = df['PostalCode'].value_counts().head(2).index.tolist()\n",
    "    if len(top_zipcodes) >= 2:\n",
    "        z1, z2 = top_zipcodes[0], top_zipcodes[1]\n",
    "        print(f\"Comparing {z1} vs {z2}...\")\n",
    "        zipcode_results.update(conduct_hypothesis_test(df[df['PostalCode'].isin([z1, z2])], 'PostalCode', alpha))\n",
    "    else:\n",
    "        print(\"Not enough unique zipcodes for comparison.\")\n",
    "    return zipcode_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea1d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Zipcode margin test\n",
    "def test_zipcode_margin_differences(df, alpha=0.05):\n",
    "    print(\"\\n--- Testing Margin Differences Between Zipcodes ---\")\n",
    "    margin_results = {}\n",
    "    top_zipcodes = df['PostalCode'].value_counts().head(2).index.tolist()\n",
    "    if len(top_zipcodes) >= 2:\n",
    "        z1, z2 = top_zipcodes[0], top_zipcodes[1]\n",
    "        print(f\"Comparing Margin for {z1} vs {z2}...\")\n",
    "        g1_margin = df[df['PostalCode'] == z1]['Margin']\n",
    "        g2_margin = df[df['PostalCode'] == z2]['Margin']\n",
    "        stat_margin, p_margin, reject_margin = perform_t_test(g1_margin, g2_margin, alpha)\n",
    "        margin_results[f'Margin_Difference_{z1}_vs_{z2}'] = {\n",
    "            'p_value': p_margin,\n",
    "            'reject_null': reject_margin,\n",
    "            'interpretation': f\"Margin difference between {z1} and {z2} is {'significant' if reject_margin else 'not significant'}\"\n",
    "        }\n",
    "    else:\n",
    "        print(\"Not enough unique zipcodes for margin comparison.\")\n",
    "    return margin_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b90d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Gender test\n",
    "def test_gender_risk_differences(df, alpha=0.05):\n",
    "    print(\"\\n--- Testing Risk Differences Between Women and Men ---\")\n",
    "    gender_results = {}\n",
    "    if 'Female' in df['Gender'].unique() and 'Male' in df['Gender'].unique():\n",
    "        print(\"Comparing Female vs Male...\")\n",
    "        gender_results.update(conduct_hypothesis_test(df[df['Gender'].isin(['Female', 'Male'])], 'Gender', alpha))\n",
    "    else:\n",
    "        print(\"Gender categories 'Female'/'Male' not found or insufficient for comparison.\")\n",
    "    return gender_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5eb820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Run all tests and display results\n",
    "province_results = test_province_risk_differences(df)\n",
    "zipcode_results = test_zipcode_risk_differences(df)\n",
    "zipcode_margin_results = test_zipcode_margin_differences(df)\n",
    "gender_results = test_gender_risk_differences(df)\n",
    "\n",
    "# Combine and display\n",
    "all_results = {\n",
    "    **province_results,\n",
    "    **zipcode_results,\n",
    "    **zipcode_margin_results,\n",
    "    **gender_results\n",
    "}\n",
    "\n",
    "for test, result in all_results.items():\n",
    "    print(f\"\\n{test}:\")\n",
    "    print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
